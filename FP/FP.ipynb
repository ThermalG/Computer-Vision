{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><h4>McGill University ECSE 415 Introduction to Computer Vision F23 - Final Project</h4>\n",
    "<h1><font color='green'>ORION: A YOLO-based Dashcam Traffic Statistical System</font></h1></center>\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ThermalG/Computer-Vision)\n",
    "\n",
    "&nbsp;\n",
    "## *¡README!*\n",
    "1. The project was developed with IntelliJ IDEA 2023.3, Python 3.11 and CUDA Toolkit 12.3. In local venv, please be advised that the GUI window will not auto pop up upon first run, so pay attention to your taskbar / dock. Also you might have to [configure the environment](https://www.jamesbowley.co.uk/qmd/opencv_cuda_python_windows.html) carefully in order to maximize the performance. Should you run in a cloud env like Colab, please first ensure appropriate adaptations, including but not limited to:\n",
    "    - Remove PyQt5 snippets and hardcode `path` to your directory with intended video(s);\n",
    "    - Uncomment the top of first cell;\n",
    "    - Optional: select GPU as runtime type;\n",
    "    - Optional: uninstall default cv2 and replace with your own built in CMake.\n",
    "2. The `PATH` variable below denotes where I store input / output videos (debugging & visualization) and other models. You may also choose whether or not you want a video ouput or simply the statistics printed on console.\n",
    "3. Only car objects (coco index 2) are considered as \"cars\"; (motor)cyclists are excluded from pedestrian count.\n",
    "\n",
    "&nbsp;\n",
    "## Change Log / To Dos\n",
    "**v0.1 20/11/2023 Prototype**\n",
    "\n",
    "**v0.2 23/11/2023**\n",
    "1. Bugfix: Solved `WARNING ⚠️ 'source' is missing. Using 'source=C:\\Users\\...` upon start.\n",
    "2. Bugfix: Solved being unable to run again after first & creating empty files without selecting any.\n",
    "3. Bugfix: Solved `AttributeError: 'NoneType' object has no attribute 'int'` for yolov8n processed at 87% .\n",
    "4. Enhancement: Allowed compression and frame extraction if runtime concern is critical.\n",
    "5. Removal: Watermark drawing and bounding boxes' complex color system.\n",
    "6. Code structure optimizized.\n",
    "\n",
    "**v0.3 30/11/2023 Submittable**\n",
    "1. <font color='orange'>Bugfix</font>: Integrate DeepSORT to suppress re-ID and duplicate detection problems.\n",
    "2. Bugfix: Enhanced the differentiation between parked and mobile vehicles. Count only passed objs now.\n",
    "3. Added real-time speed estimation using optical flow.\n",
    "4. Added a buffer mechanism in optical flow calculation to potentially improve efficiency.\n",
    "5. <font color='yellow'>Enhancement</font>: Simple GUI with multithreading / batch processing support.\n",
    "6. <font color='yellow'>Enable GPU acceleration</font>: TensorRT, OpenCV with CUDA and more.\n",
    "7. <font color='red'>Finetune parameters</font> (search ##).\n",
    "8. Minor code optimizations.\n",
    "9. As instructed, bus and truck are eliminated from 'cars' category.\n",
    "10. <font color='orange'>Bugfix</font>: Take advantage of optical flow to improve cyclist recognition.\n",
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3beec0a98da748eb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install ultralytics\n",
    "# !pip install deep-sort-realtime\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "\n",
    "PATH = './Data/'\n",
    "WRITE_VIDEO = True # choose if you need a processed video (saving to PATH) or just statistics\n",
    "app = QApplication.instance() or QApplication([])   # QApp should only be created once"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T20:54:21.212695400Z",
     "start_time": "2023-12-06T20:54:16.880894900Z"
    }
   },
   "id": "9019ad554028f3f7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# REF[4][5][8]\n",
    "class OF_Buffer:    #! SEMI-FINISHED\n",
    "    def __init__(self, size = 5):\n",
    "        self.size = size\n",
    "        self.frames = []\n",
    "        self.flow_data = []\n",
    "    def add(self, frame, flow):\n",
    "        if len(self.frames) >= self.size:\n",
    "            self.frames.pop(0)\n",
    "            self.flow_data.pop(0)\n",
    "        self.frames.append(frame)\n",
    "        self.flow_data.append(flow)\n",
    "    def getter(self):   # get the latest frame and its flow data\n",
    "        if self.frames:\n",
    "            return self.frames[-1], self.flow_data[-1]\n",
    "        return None, None\n",
    "\n",
    "# buffer = OF_Buffer(size = 10) # depending on available memory. adjust with S, FRAME_PARKED, FRAME_CYCLIST\n",
    "def OF(frame_prev, frame, gpu = False): # for GPU, only NVIDIA platforms supported\n",
    "    if gpu and cv2.cuda.getCudaEnabledDeviceCount():\n",
    "        g1 = cv2.cuda.cvtColor(cv2.cuda_GpuMat().upload(frame_prev), cv2.COLOR_BGR2GRAY)\n",
    "        g2 = cv2.cuda.cvtColor(cv2.cuda_GpuMat().upload(frame), cv2.COLOR_BGR2GRAY)\n",
    "        gpu_flow = cv2.cuda_FarnebackOpticalFlow.create(.5, False, 15, 3, 5, 1.2, 0)\n",
    "        flow = gpu_flow.calc(g1, g2, None).download()\n",
    "    else:\n",
    "        g1, g2 = cv2.cvtColor(frame_prev, cv2.COLOR_BGR2GRAY), cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(g1, g2, None, .5, 3, 15, 3, 5, 1.2, 0)  ##\n",
    "    # buffer.add(frame, flow)\n",
    "    return cv2.cartToPolar(flow[..., 0], flow[..., 1])[0] # magnitude\n",
    "\n",
    "def radar(height, f):  # REF[9]. not used; can help differentiate between parked and mobile vehicles\n",
    "    \"\"\"\n",
    "    Estimate the distance of a vehicle from a monocular camera system\n",
    "        :param f: Focal length in mm.\n",
    "        :param height: Real height of the viewpoint vehicle in mm.\n",
    "        :return: Estimated distance in meters.\n",
    "    \"\"\"\n",
    "    pixel_h = y2 - y1   # apparent height of the detected vehicle in picture\n",
    "    return (f * height) / pixel_h / 1000    # D = (F * W) / P, converted to meters\n",
    "\n",
    "def marker(img, lbl = '', status = '', clr = (255, 255, 0)): # REF[6]\n",
    "    W, H = x2 - x1, y2 - y1\n",
    "    # dynamic text relevant to box size (alternatively, use radar())\n",
    "    size = max(.5, min(1.5, .005 * (H * W)**.5)) * C\n",
    "    L = min(W, H) // 8  # L-shaped corner size\n",
    "    for (x, y) in [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]:\n",
    "        cv2.line(img, (x, y), (x + (L if x == x1 else -L), y), clr, int(4 * C))\n",
    "        cv2.line(img, (x, y), (x, y + (L if y == y1 else -L)), clr, int(4 * C))\n",
    "    if lbl:\n",
    "        w, h = cv2.getTextSize(lbl, 0, size, 1)[0]\n",
    "        coord = y1 - h - 1 if y1 - h - 1 >= h else y1 + h + 1\n",
    "        cv2.putText(img, lbl, (x1, coord), 0, size, (0, 155, 255), int(2 * C))\n",
    "    if status:\n",
    "        w, h = cv2.getTextSize(status, 0, .7 * size, 1)[0]\n",
    "        x = max(0, min(x2 - w, img.shape[1] - w))\n",
    "        y = max(h, min(y2 + h + 10, img.shape[0] - h))\n",
    "        cv2.putText(img, status, (x, y), 0, .7 * size, c, int(2 * C))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T20:54:21.806704100Z",
     "start_time": "2023-12-06T20:54:21.790909500Z"
    }
   },
   "id": "e5e7ff6ffcec3af8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROGRESS: 100%|██████████| 1071/1071 [01:41<00:00, 10.55frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOBILE: 44\n",
      "PARKED: 0\n",
      "PEDESTRIANS: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS & THRESHOLDS\n",
    "C = 1.0                 # image sacling factor. primitve res 2562×1440\n",
    "S = 1                   # frame extraction freq. effective frame rate 30 / S, starting from the first\n",
    "assert 0 < C <= 1, \"COMPRESSION COEFFICIENT MUST LIE WITHIN (0, 1]\"\n",
    "assert S > 0 and isinstance(S, int), \"FRAME SAMPLING INTERVAL MUST BE A POSITIVE INTEGER\"\n",
    "PROXIMITY = 100 * C     # proximity threshold between a person and a bike\n",
    "CONTIGUITY = 4 // S     # consecutive frames for a car/person to be considered parked/cyclist\n",
    "DELTA = 3.6 * C / S     # correction factor for speed estimation in km/h. REF[5][8]\n",
    "ROI = .5                # % of which most movements are expected in pictures. primarily varied with cam angle\n",
    "\n",
    "# INITIALIZATION\n",
    "detector = YOLO(PATH + 'yolov8n.pt')    # REF[2]\n",
    "# tracker = DeepSort(PATH + 'mars-small128.ckpt-68577', nms_max_overlap = .5, max_age = 60, nn_budget = 100)\n",
    "path, _ = QFileDialog.getOpenFileName(None, \"Orion by ThermalG\", PATH, \"File types (*.mp4 *.avi *.mov)\")\n",
    "if not path:    # REF[7]\n",
    "    print(\"NO FILE SELECTED. PROGRAM TERMINATED.\")\n",
    "    sys.exit()\n",
    "source = cv2.VideoCapture(path)\n",
    "size = (int(source.get(cv2.CAP_PROP_FRAME_WIDTH) * C), int(source.get(cv2.CAP_PROP_FRAME_HEIGHT) * C))\n",
    "if WRITE_VIDEO:\n",
    "    fps = int(source.get(cv2.CAP_PROP_FPS)) # primitve FPS 29.97\n",
    "    # noinspection PyArgumentList\n",
    "    codec = cv2.VideoWriter_fourcc(*'MP4V') #! X264 can take double storage with compromised output\n",
    "    output = cv2.VideoWriter(path.rsplit('.', 1)[0] + '_output.mp4', codec, fps / S, size)\n",
    "v_stack, p_stack, b_stack, cyclist = (defaultdict(list) for _ in range(4))   # tracking history init\n",
    "parked, v_lastSeen, p_lastSeen = (defaultdict(int) for _ in range(3))\n",
    "v_ids, p_ids = {}, {}\n",
    "summary, speed_trap = [], []\n",
    "frame_prev = None\n",
    "ctr = 0 # frame counter\n",
    "s = 0   # current speed\n",
    "\n",
    "# frame-wise Main Loop\n",
    "with tqdm(total = math.ceil(source.get(cv2.CAP_PROP_FRAME_COUNT) / S), desc = \"PROGRESS\", unit = \"frame\") as pbar:\n",
    "    while source.isOpened():    # REF[1][3]\n",
    "        read, frame = source.read()\n",
    "        if not read:  # read failure (file corrupted) or finished\n",
    "            break\n",
    "        if ctr % S == 0:\n",
    "            frame = cv2.resize(frame, size)\n",
    "            results = detector.track(frame, conf = .5, persist = True, verbose = False) ##\n",
    "            if frame_prev is not None:  # REF[4][5]\n",
    "                mag = OF(frame_prev, frame)\n",
    "                ## weight-average for smoothing at the cost of latency showing speed change (not affecting max)\n",
    "                speed_trap.append((s := .9 * s + .1 * DELTA * np.mean(mag[int(ROI * mag.shape[0]):])))\n",
    "                cv2.putText(frame, f'Speed: {s:.1f} KPH', (int(.05 * size[0]), int(.05 * size[1])), 2, 2 * C, (255, 0, 127), int(2 * C)) if WRITE_VIDEO else None\n",
    "            if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "                for id, box in zip(results[0].boxes.id.int().cpu().tolist(), results[0].boxes.data):\n",
    "                    x1, y1, x2, y2 = map(int, box[:4])\n",
    "                    idx = int(box[-1])\n",
    "                    if idx == 2:    # car category\n",
    "                        v_lastSeen[id] = ctr\n",
    "                        if id not in v_ids:\n",
    "                            v_ids[id] = len(v_ids)\n",
    "                        pos = (box[0], box[1])\n",
    "                        pos_prev = v_stack[id][-1] if id in v_stack else pos\n",
    "                        v_stack[id].append(pos)\n",
    "                        # monitor vehicle movement\n",
    "                        dist = math.hypot(abs(pos_prev[0] - pos[0]) + abs(pos_prev[1] - pos[1]))\n",
    "                        diff = 5 * C * S * s / DELTA    ##\n",
    "                        if dist < 1.5 * diff:\n",
    "                            if dist < .05 * diff:\n",
    "                                # mobile in the same lane or parked far ahead. not passed thus not interested\n",
    "                                status = 'INDECISIVE'\n",
    "                            else:\n",
    "                                parked[id] += 1\n",
    "                                status = 'PARKED' if parked[id] > CONTIGUITY else 'INDECISIVE'\n",
    "                        else:\n",
    "                            parked[id] = 0\n",
    "                            status = 'MOBILE'\n",
    "                        c = (0, 0, 255) if status == 'MOBILE' else (0, 255, 0) if status == 'PARKED' else (0, 0, 0)\n",
    "                        marker(frame, f'CAR{[idx]} {v_ids[id] + 1}', status, c) if WRITE_VIDEO else None\n",
    "                    elif idx == 0 and frame_prev is not None:   # person category\n",
    "                        p_lastSeen[id] = ctr\n",
    "                        pos = (box[0], box[1])\n",
    "                        p_stack[id].append(pos)\n",
    "                        if id not in p_ids and id not in cyclist:\n",
    "                            p_ids[id] = len(p_ids)\n",
    "                        marker(frame, f'PEDESTRIAN {p_ids[id] + 1}') if WRITE_VIDEO else None\n",
    "                    elif idx in [1, 3]:     # bicycle and motorcycle categories\n",
    "                        b_stack[id].append((box[0], box[1]))\n",
    "                for b_id, p_pos in p_stack.items(): # rule out cyclists\n",
    "                    for b_pos in b_stack.items():   # REF[6]\n",
    "                        if len(b_pos) >= CONTIGUITY and len(p_pos) >= CONTIGUITY and all(abs(p_pos[-i][0] - b_pos[-i][0].item()) + abs(p_pos[-i][1] - b_pos[-i][1].item()) < PROXIMITY for i in range(1, CONTIGUITY + 1)):\n",
    "                            cyclist.add(b_id)\n",
    "                            break\n",
    "            # update counts\n",
    "            summary = [f'MOBILE: {max(0, len([v for v, f in v_lastSeen.items() if f < ctr]) - sum(parked[v] > CONTIGUITY for v in parked))}',\n",
    "                       f'PARKED: {sum(parked[v] > CONTIGUITY for v in parked)}',\n",
    "                       f'PEDESTRIANS: {len([p for p, f in p_lastSeen.items() if f < ctr]) - len(cyclist)}']\n",
    "            if WRITE_VIDEO:\n",
    "                for i, info in enumerate(summary, start = 1):\n",
    "                    cv2.putText(frame, info, (int(.8 * size[0]), int(.03 * i * size[1])), 0, C, (255, 0, 0), int(2 * C))\n",
    "                # noinspection PyUnboundLocalVariable\n",
    "                output.write(frame)\n",
    "            pbar.update(1)\n",
    "            frame_prev = frame.copy()   # previous frame now becomes the current\n",
    "        ctr += 1\n",
    "\n",
    "print('\\n'.join(summary))\n",
    "print(f'Interval Speed Maximum: {max(speed_trap):.1f} KPH')\n",
    "source.release()\n",
    "if WRITE_VIDEO:\n",
    "    output.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T21:16:51.703370300Z",
     "start_time": "2023-12-06T21:15:08.290701700Z"
    }
   },
   "id": "8f84b559ffdadb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><h2>PROJECT REPORT</h2></center>\n",
    "\n",
    "#### Introduction - Procecure Overview\n",
    "ORION System is designed to analyze dashcam videos to provide detailed analytics on moving and parked cars, as well as pedestrians. It employs techniques including YOLO for object detection and Farneback for optical flow calculations. The project aims to offer insights into traffic patterns and potentially improve dashcam technology for practical applications like traffic warning and driver assistance systems.\n",
    "\n",
    "&nbsp;\n",
    "#### Main - Performance Evaluation\n",
    "The system was evaluated against manually obtained ground truth values for two dashcam videos.<br>\n",
    "**Ground Truth** (very unlikely accurate, especially pedestrians in st-catherines_drive.mp4)\n",
    "\n",
    "| counts        | mcgill_drive | st-catherines_drive |\n",
    "|---------------|--------------|---------------------|\n",
    "| mobile passed | 24           | 0                   |\n",
    "| parked passed | 16           | 59                  |\n",
    "| pedestrians   | 35           | 108                 |\n",
    "\n",
    "**Predicted**\n",
    "\n",
    "| counts        | mcgill_drive | st-catherines_drive |\n",
    "|---------------|--------------|---------------------|\n",
    "| mobile passed | 24           | 8                   |\n",
    "| parked passed | 20           | 60                  |\n",
    "| pedestrians   | 26           | 67                  |\n",
    "\n",
    "As for speed, please refer to the prints above directly. My eyes are not speedometer so ground truth seems impossible.\n",
    "\n",
    "**Efficiency**: In the current version, our program is quite inefficient. The majority of computational stress lies in calculating the optical flow (on CPU, GPU idling most of the time), which is only used for the bonus functionality - speed estimation. Eliminating it can boost the speed by 10 times (to around 12 fps on 1650S + 12600K). Further, if video output unnecessary and just to meet the basic requirements, you can set WRITE_VIDEO to False, bringing speed to 18 fps. We also observed that even without optical flow, the avg GPU usage is only 30%. Finishing GPU acceleration can easily boost above 30 fps with any mid-range GPUs to achieve real-time processing.\n",
    "\n",
    "**Accuracy**: We dedicated most effort to accuracy details: exluding cyclists, differentiating parked and mobile cars, avoiding re-ID and duplication, etc. In the tables above, you can see our system is able to achieve 90% accuracy on the given videos. However, it is not robust enough to handle different traffic scenarios, since `diff` remains constant for different speed and traffic condition. Also, you may have noticed some cyclists are still detected occasionally. This is probably due to the relatively poor detection ability of the yolov8n model we chose. In these frames, the bicycles, which the exclusion algo relies on, are not detected because they are of irregualr shapes or massively blocked by the cyclists. Last, the biggest disparency is that predicted pedestrian count always seems less. Such cpnsequences are very much expected because even when we were determining ground truth with shrewd eyes, some person objects are so secluded. Using larger models instead of nano can improve the situation, but lead to other more intractable problems, like detecting reflected objects on glass, non-person objects like the sculpture at Peel-Sherbrooke. However, nano does have its own glitches. For instance, it recognizes man on the mailbox poster and misses the excavator. But balancing with runtime, nano proves worthy. We believe these problems can be solved by utilizing our optical flow algo.\n",
    "\n",
    "**Robustness**: Robustness has been improved to handle all video file types and all kinds of anticipated bugs during processing (like if no objects detected in a frame). We also integrated a frame extraction and video compression mechnism to speed up the processing if needed. There is no need to waste words on elaborating this part. You can refer to our change log at the beginning.\n",
    "\n",
    "**Cross-Nominal Comparison**: After reviewing some similar road monitor system on github and youtube, we are confident that our model is doing the job quite well.\n",
    "\n",
    "&nbsp;\n",
    "#### Overall Approach\n",
    "The program aims to detect, track, and classify cars and pedestrians (trucks, buses, cyclists can be easily added), and calculate the subjective car's speed. Below is the methodology walkthrough.\n",
    "\n",
    "1. **Deep Learning for Object Detection and Tracking**\n",
    "Utilizes the YOLO (You Only Look Once) deep learning model for real-time object detection.\n",
    "Applies DeepSort for tracking objects across frames (yet to be integrated).\n",
    "\n",
    "2. **Optical Flow for Motion Analysis**\n",
    "Implements two methods for calculating optical flow (OF_NV and OF, combined in the newest version), which is a technique to estimate motion between video frames. The CPU version uses a traditional method with cv2.calcOpticalFlowFarneback. This is the default method in this assignment and is extremely slow. GPU implementation using RAFT (Recurrent All-Pairs Field Transforms for Optical Flow) [10] was implemented and discarded: 100% GPU usage but barely half speed of CPU version.\n",
    "\n",
    "3. **Data Processing and Analysis**\n",
    "Frames are processed at a specific frequency, determined by the S variable.\n",
    "For each frame, object detection is performed to identify cars, pedestrians, bicycles, etc.\n",
    "Optical flow data is used to analyze motion and compute the vehicle's speed.\n",
    "\n",
    "4. **Specific Object Detection and Behavior Analysis**\n",
    "The code differentiates between cars, pedestrians, and cyclists using both detection labels and motion patterns.\n",
    "It identifies parked cars by tracking their movement over time and checking if the movement is below a certain threshold.\n",
    "Cyclists are identified using a combination of detection labels and movement patterns that are characteristic of cycling.\n",
    "\n",
    "5. **Speed Estimation**\n",
    "The vehicle's speed is estimated using optical flow data. This involves calculating the average flow of pixels in a specified region of interest and applying a correction factor for accuracy.\n",
    "\n",
    "6. **Visual Annotations and Output**\n",
    "The code can optionally output a processed video with visual annotations, like bounding boxes, labels (e.g., 'CAR', 'PEDESTRIAN'), and the vehicle's speed.\n",
    "It uses functions like cv2.putText and cv2.line to draw on the video frames.\n",
    "\n",
    "7. **Data Structures for Tracking and Analysis**\n",
    "Uses dictionaries and lists to store tracking data and history for each detected object.\n",
    "Maintains counts of different types of objects and their states (e.g., parked vs. mobile cars).\n",
    "\n",
    "8. **User Interface and File Handling (commented out)**\n",
    "The code includes a segment for file selection through a GUI using QFileDialog.\n",
    "Handles video files, reading frames from them, and potentially saving the processed output.\n",
    "\n",
    "**Assumptions**<br>\n",
    "Video Source: Assumes the input is video footage, likely from a vehicle-mounted camera.\n",
    "CUDA Support: Assumes the availability of CUDA-enabled devices for GPU acceleration.\n",
    "Environment Setup: Assumes a Python environment with necessary dependencies installed.\n",
    "File Paths and Formats: Assumes specific file paths and formats for model weights and video files.\n",
    "GUI Components: Assumes the use of a graphical user interface for file selection.\n",
    "\n",
    "&nbsp;\n",
    "#### Description of Used Software Package & Routine\n",
    "**Ultralytics YOLO**: A deep learning model for real-time object detection. It's used here to detect objects like cars, pedestrians, and cyclists in each video frame.\n",
    "**PyQt5**: A set of Python bindings for Qt application framework, used for GUI elements like file dialogs.\n",
    "**TQDM**: A library providing progress bars for loops and other iterative processes.\n",
    "*The rest of packages imported are rather common. No need to elaborate.*\n",
    "\n",
    "&nbsp;\n",
    "#### Conclusion - Future Work\n",
    "The ORION system demonstrates promising capabilities in traffic analytics using computer vision. However, it is semi-finished and our future work can focus on:\n",
    "\n",
    "Improving Accuracy: Enhancing the model's ability to differentiate between closely spaced objects and in diverse lumiantion conditions. This means multiple optimizations to the current model, especially finishing the integration of DeepSORT to suppress re-ID and duplicate detection problems.\n",
    "Real-Time Processing: Finish the GPU acceleration algorithms for faster processing to support real-time analytics.\n",
    "Extended Functionality: Finish the distance measuring system as an auxiliary algorithm to differentiate between parked and mobile vehicles.\n",
    "User Interface: Curently we comment out the GUI for video selecting and hardcoded it to just process the two interested videos, since this is an assignment so we should make it grading-friendly in one notebook. In the future, we might want to develop an intuitive interface for users to interact with the system and access analytics more efficiently.\n",
    "\n",
    "&nbsp;\n",
    "## References\n",
    "1. **Zhao, C.** (2023, September 14). *YOLOv8 based target tracking - car tracking and counting*.\n",
    "    - [CSDN](https://blog.csdn.net/zhaocj/article/details/132800296)\n",
    "    - accessed: 19/11/2023\n",
    "2. **Ultralytics YOLOv8 Docs**. (n.d.). *Multi-Object Tracking with Ultralytics YOLO*.\n",
    "    - [Ultralytics](https://docs.ultralytics.com/modes/track/)\n",
    "    - accessed: 19/11/2023\n",
    "3. **Moin, M.** (2023, January 25). *YOLOv8-DeepSORT-Object-Tracking*.\n",
    "    - [GitHub](https://github.com/MuhammadMoinFaisal/YOLOv8-DeepSORT-Object-Tracking)\n",
    "    - accessed: 21/11/2023\n",
    "4. **Kuklin, M.** (2021, July 16). *Optical Flow in OpenCV (C++/Python)*.\n",
    "    - [LearnOpenCV](https://learnopencv.com/optical-flow-in-opencv/)\n",
    "    - accessed: 22/11/2023\n",
    "5. **Shafu0x**. (n.d.). *Vehicle Speed Estimation from Video using Deep Learning and Optical Flow in PyTorch*.\n",
    "    - [GitHub](https://github.com/shafu0x/vehicle-speed-estimation)\n",
    "    - accessed: 22/11/2023\n",
    "6. **How to detect if object is stationary OpenCV**. (n.d.).\n",
    "    - [Stack Overflow](https://stackoverflow.com/questions/68683164/how-to-detect-if-object-is-stationary-opencv)\n",
    "    - accessed: 22/11/2023\n",
    "7. **PyQT Documentation v5.15.4**. (n.d.).\n",
    "    - [QTWidgets](https://www.riverbankcomputing.com/static/Docs/PyQt5/api/qtwidgets/qtwidgets-module.html)\n",
    "    - accessed: 20/11/2023\n",
    "8. **Object for estimating optical flow using Farneback method**. (n.d.).\n",
    "    - [MATLAB](https://www.mathworks.com/help/vision/ref/opticalflowfarneback.html)\n",
    "    - accessed: 23/11/2023\n",
    "9. **yolov8-车辆测距+追尾预警+车辆检测识别+车辆跟踪测速**。 (n.d.). \n",
    "    - [CSDN](https://blog.csdn.net/weixin_44944382/article/details/128518452)\n",
    "    - accessed: 26/11/2023\n",
    "10. **Teed, Z., & Deng, J.** (2020). Raft: Recurrent all-pairs field transforms for optical flow. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16* (pp. 402-419). Springer International Publishing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "170c28a8b97bf8e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
